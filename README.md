# Circuit Fault Classification (RC / RL / RLC)

## 1) Project overview  ✅
This project implements an **end-to-end machine learning pipeline** to classify **circuit faults** (e.g., RC/RL/RLC-related classes) from numeric features extracted from circuit responses.

You can use either:
- an **existing labeled CSV dataset**, or
- **(optional)** generate a dataset via **NGSpice/PySpice simulation**.

The pipeline outputs:
- processed splits (`.npy`) + label mapping (`label_map.json`)
- trained models (SVM / k-NN / shallow neural network)
- evaluation results (confusion matrix plots + JSON metrics)

---

## 2) Core functions (collaboration scheme summary) ✅
The code is organized as a **collaborating script pipeline**. Each module has a clear role:

### 2.1 Collaboration flow (how scripts work together)
```
(OPTIONAL) Simulate_Circuit.py
        │  generates simulations.csv (raw numeric features)
        ▼
Data_Preprocessing.py
        │  reads dataset CSV
        │  detects label column + encodes labels
        │  splits train/val/test + standardizes features
        │  saves data_processed/*
        ▼
Train_Models.py
        │  loads data_processed/* (scaled arrays)
        │  trains models + saves artifacts/*
        ▼
Evaluation.py
        │  loads latest models from artifacts/*
        │  evaluates + saves results/*
```

### 2.2 What each script does (core functions)
- **`Simulate_Circuit.py`**  
  *Optional.* Generates circuit response samples using PySpice + NGSpice and saves `data/raw/simulations.csv`.

- **`Data_Preprocessing.py`**  
  Reads a CSV dataset, auto-detects a label column, builds numeric feature arrays, performs train/val/test split, applies `StandardScaler`, and saves:
  - `data_processed/X_train.npy`, `X_val.npy`, `X_test.npy`
  - `data_processed/y_train.npy`, `y_val.npy`, `y_test.npy`
  - `data_processed/scaler_params.npz`
  - `data_processed/label_map.json`
  - `data_processed/class_weights.npy`

- **`Train_Models.py`**  
  Loads the processed splits and trains:
  - **SVM** (GridSearchCV) → saved as `artifacts/svm_*.joblib`
  - **k-NN** (GridSearchCV) → saved as `artifacts/knn_*.joblib`
  - **Shallow NN** (SciKeras / RandomizedSearchCV) → saved as `artifacts/shallow_nn_*.keras`

- **`Evaluation.py`**  
  Loads the **latest models** from `artifacts/` and saves:
  - confusion matrix figures
  - a JSON file of metrics per model

- **`Feature_Extraction.py`**  
  Optional utilities for handcrafted features (FFT/PSD/peaks, etc.) if you want to extend the dataset.

- **`experiment_utils.py`**  
  Small helper functions for organizing experiments (not required for the default pipeline).

- **`run_experiment.py`**  
  Contains a placeholder section (`...`) and is **not runnable** until you fill the USER INPUT block.

---

## 3) Environment setup steps (VS Code → ready to run) ✅

### 3.1 Install Python
Install **Python 3.10+** (recommended: 3.10 or 3.11).  
During installation, check **“Add Python to PATH”**.

Verify:
```bash
python --version
pip --version
```

### 3.2 Open the project in VS Code
VS Code → **File → Open Folder…** → select your project folder.

### 3.3 Create and activate a virtual environment (recommended)
Open VS Code terminal: **Terminal → New Terminal**

Windows (PowerShell):
```bash
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

Windows (CMD):
```bat
python -m venv .venv
.\.venv\Scripts\activate
```

### 3.4 Select interpreter in VS Code
`Ctrl+Shift+P` → **Python: Select Interpreter** → choose `.venv`

### 3.5 Install dependencies
```bash
pip install -U pip
pip install numpy pandas scikit-learn scipy matplotlib joblib scikeras tensorflow
```

#### Optional: simulation dependencies (PySpice + NGSpice)
Only needed if you will run `Simulate_Circuit.py`:
```bash
pip install PySpice
```

Install **NGSpice** and ensure it’s on PATH.  
`Simulate_Circuit.py` currently appends:
```python
os.environ["PATH"] += r";C:\ngspice\bin"
```
So either install NGSpice to `C:\ngspice\bin` or edit that path.

---

## 4) Directory structure explanation ✅
The scripts assume a root directory like:

```
C:\Project\Code
├─ Data-set's\
│  └─ circuit_fault_dataset-1.csv          # input dataset (required unless simulating)
├─ data\raw\
│  └─ simulations.csv                       # optional (generated by Simulate_Circuit.py)
├─ data_processed\                          # generated by Data_Preprocessing.py
│  ├─ X_train.npy, X_val.npy, X_test.npy
│  ├─ y_train.npy, y_val.npy, y_test.npy
│  ├─ scaler_params.npz
│  ├─ class_weights.npy
│  └─ label_map.json
├─ artifacts\                               # generated by Train_Models.py
│  ├─ svm_YYYYMMDD_HHMMSS.joblib
│  ├─ knn_YYYYMMDD_HHMMSS.joblib
│  ├─ shallow_nn_YYYYMMDD_HHMMSS.keras
│  └─ *_evaluation_YYYYMMDD_HHMMSS.json
└─ results\                                 # generated by Evaluation.py
   ├─ class_distribution.png
   ├─ svm_confusion_matrix.png
   ├─ knn_confusion_matrix.png
   └─ shallow_nn_confusion_matrix.png
```

### Important: update paths if your folder is different
By default, these scripts use `C:\Project\Code`.
If your project is elsewhere, update:
- `PROJECT_ROOT` in `Data_Preprocessing.py`, `Train_Models.py`, `Simulate_Circuit.py`
- `BASE_DIR` in `Evaluation.py`

---

## 5) Quick-start commands (train + infer examples) ✅

### 5.1 Training quick-start (recommended order)

**(A) Put your dataset CSV in:**
```
C:\Project\Code\Data-set's\circuit_fault_dataset-1.csv
```

**(B) Preprocess**
```bash
python Data_Preprocessing.py
```

**(C) Train**
```bash
python Train_Models.py
```

**(D) Evaluate**
```bash
python Evaluation.py
```

---

### 5.2 Inference example (predict on new samples)

#### Option 1 — Inference with SVM / k-NN (.joblib)
These are sklearn `Pipeline`s that already include scaling.  
Create a small script (example `infer_joblib.py`) and run it:

```python
import joblib
import numpy as np
import pandas as pd

MODEL_PATH = r"C:\Project\Code\artifacts\svm_YYYYMMDD_HHMMSS.joblib"  # change to your file
CSV_PATH   = r"C:\Project\Code\Data-set's\circuit_fault_dataset-1.csv"

df = pd.read_csv(CSV_PATH)

# IMPORTANT: Evaluation.py uses the first non-numeric column as label.
# For inference, drop any non-numeric column(s) and keep only numeric features:
X = df.select_dtypes(include=["number"]).values

model = joblib.load(MODEL_PATH)
pred = model.predict(X[:5])  # first 5 rows
print("Predicted class indices:", pred)
```

Run:
```bash
python infer_joblib.py
```

#### Option 2 — Inference with shallow NN (.keras)
The NN is trained on **scaled arrays saved by Data_Preprocessing.py**.  
So you must apply the same scaler before prediction.

```python
import json
import numpy as np
import pandas as pd
from tensorflow.keras.models import load_model

BASE_DIR = r"C:\Project\Code"
MODEL_PATH = rf"{BASE_DIR}\artifacts\shallow_nn_YYYYMMDD_HHMMSS.keras"  # change
SCALER_NPZ = rf"{BASE_DIR}\data_processed\scaler_params.npz"
LABEL_MAP  = rf"{BASE_DIR}\data_processed\label_map.json"

# Load label map (original_label -> index)
with open(LABEL_MAP, "r") as f:
    label_map = json.load(f)
inv_label_map = {v: k for k, v in label_map.items()}

# Load scaler parameters
sc = np.load(SCALER_NPZ)
mean, scale = sc["mean"], sc["scale"]

# Load any CSV and keep numeric features
df = pd.read_csv(rf"{BASE_DIR}\Data-set's\circuit_fault_dataset-1.csv")
X = df.select_dtypes(include=["number"]).values.astype("float32")

# Apply standardization: (X - mean) / scale
X_scaled = (X - mean) / scale

model = load_model(MODEL_PATH)
proba = model.predict(X_scaled[:5])
pred_idx = np.argmax(proba, axis=1)
pred_labels = [inv_label_map[int(i)] for i in pred_idx]

print("Predicted indices:", pred_idx)
print("Predicted labels:", pred_labels)
```

---

## 6) Troubleshooting

### VS Code can’t find packages
Make sure VS Code uses `.venv`:
`Ctrl+Shift+P → Python: Select Interpreter → .venv`

### TensorFlow install problems on Windows
```bash
pip install --upgrade pip setuptools wheel
pip install tensorflow
```

### NGSpice not found (simulation)
Update the NGSpice path in `Simulate_Circuit.py`:
```python
os.environ["PATH"] += r";C:\ngspice\bin"
```

### Label column issues
- `Data_Preprocessing.py` prefers label columns: `label`, `fault_label`, `class`, `target`
- If your label column is different, force it in code or rename the CSV column.

---

## 7) One-command pipeline (after setup)
```bash
python Data_Preprocessing.py
python Train_Models.py
python Evaluation.py
```
